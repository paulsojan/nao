"""Database syncing functionality for generating markdown documentation from database schemas."""

import shutil
from collections import defaultdict
from pathlib import Path
from typing import Dict, List

from rich.console import Console
from rich.progress import BarColumn, Progress, SpinnerColumn, TaskProgressColumn, TextColumn

from .accessors import DataAccessor
from .registry import get_accessors

console = Console()


def sync_bigquery(
    db_config,
    base_path: Path,
    progress: Progress,
    accessors: list[DataAccessor],
) -> tuple[int, int]:
    """Sync BigQuery database schema to markdown files.

    Args:
            db_config: The database configuration
            base_path: Base output path
            progress: Rich progress instance
            accessors: List of data accessors to run

    Returns:
            Tuple of (datasets_synced, tables_synced)
    """
    conn = db_config.connect()
    db_path = base_path / "type=bigquery" / f"database={db_config.project_id}"

    datasets_synced = 0
    tables_synced = 0

    if db_config.dataset_id:
        datasets = [db_config.dataset_id]
    else:
        datasets = conn.list_databases()

    dataset_task = progress.add_task(
        f"[dim]{db_config.name}[/dim]",
        total=len(datasets),
    )

    for dataset in datasets:
        try:
            all_tables = conn.list_tables(database=dataset)
        except Exception:
            progress.update(dataset_task, advance=1)
            continue

        # Filter tables based on include/exclude patterns
        tables = [t for t in all_tables if db_config.matches_pattern(dataset, t)]

        # Skip dataset if no tables match
        if not tables:
            progress.update(dataset_task, advance=1)
            continue

        dataset_path = db_path / f"schema={dataset}"
        dataset_path.mkdir(parents=True, exist_ok=True)
        datasets_synced += 1

        table_task = progress.add_task(
            f"  [cyan]{dataset}[/cyan]",
            total=len(tables),
        )

        for table in tables:
            table_path = dataset_path / f"table={table}"
            table_path.mkdir(parents=True, exist_ok=True)

            for accessor in accessors:
                content = accessor.generate(conn, dataset, table)
                output_file = table_path / accessor.filename
                output_file.write_text(content)

            tables_synced += 1
            progress.update(table_task, advance=1)

        progress.update(dataset_task, advance=1)

    return datasets_synced, tables_synced


def sync_duckdb(
    db_config,
    base_path: Path,
    progress: Progress,
    accessors: list[DataAccessor],
) -> tuple[int, int]:
    """Sync DuckDB database schema to markdown files.

    Args:
            db_config: The database configuration
            base_path: Base output path
            progress: Rich progress instance
            accessors: List of data accessors to run

    Returns:
            Tuple of (schemas_synced, tables_synced)
    """
    conn = db_config.connect()

    # Derive database name from path
    if db_config.path == ":memory:":
        db_name = "memory"
    else:
        db_name = Path(db_config.path).stem

    db_path = base_path / "type=duckdb" / f"database={db_name}"

    schemas_synced = 0
    tables_synced = 0

    # List all schemas in DuckDB
    schemas = conn.list_databases()

    schema_task = progress.add_task(
        f"[dim]{db_config.name}[/dim]",
        total=len(schemas),
    )

    for schema in schemas:
        try:
            all_tables = conn.list_tables(database=schema)
        except Exception:
            progress.update(schema_task, advance=1)
            continue

        # Filter tables based on include/exclude patterns
        tables = [t for t in all_tables if db_config.matches_pattern(schema, t)]

        # Skip schema if no tables match
        if not tables:
            progress.update(schema_task, advance=1)
            continue

        schema_path = db_path / f"schema={schema}"
        schema_path.mkdir(parents=True, exist_ok=True)
        schemas_synced += 1

        table_task = progress.add_task(
            f"  [cyan]{schema}[/cyan]",
            total=len(tables),
        )

        for table in tables:
            table_path = schema_path / f"table={table}"
            table_path.mkdir(parents=True, exist_ok=True)

            for accessor in accessors:
                content = accessor.generate(conn, schema, table)
                output_file = table_path / accessor.filename
                output_file.write_text(content)

            tables_synced += 1
            progress.update(table_task, advance=1)

        progress.update(schema_task, advance=1)

    return schemas_synced, tables_synced


def sync_databricks(
    db_config,
    base_path: Path,
    progress: Progress,
    accessors: list[DataAccessor],
) -> tuple[int, int]:
    """Sync Databricks database schema to markdown files.

    Args:
            db_config: The database configuration
            base_path: Base output path
            progress: Rich progress instance
            accessors: List of data accessors to run

    Returns:
            Tuple of (schemas_synced, tables_synced)
    """
    conn = db_config.connect()
    catalog = db_config.catalog or "main"
    db_path = base_path / "type=databricks" / f"database={catalog}"

    schemas_synced = 0
    tables_synced = 0

    if db_config.schema:
        schemas = [db_config.schema]
    else:
        schemas = conn.list_databases()

    schema_task = progress.add_task(
        f"[dim]{db_config.name}[/dim]",
        total=len(schemas),
    )

    for schema in schemas:
        try:
            all_tables = conn.list_tables(database=schema)
        except Exception:
            progress.update(schema_task, advance=1)
            continue

        # Filter tables based on include/exclude patterns
        tables = [t for t in all_tables if db_config.matches_pattern(schema, t)]

        # Skip schema if no tables match
        if not tables:
            progress.update(schema_task, advance=1)
            continue

        schema_path = db_path / f"schema={schema}"
        schema_path.mkdir(parents=True, exist_ok=True)
        schemas_synced += 1

        table_task = progress.add_task(
            f"  [cyan]{schema}[/cyan]",
            total=len(tables),
        )

        for table in tables:
            table_path = schema_path / f"table={table}"
            table_path.mkdir(parents=True, exist_ok=True)

            for accessor in accessors:
                content = accessor.generate(conn, schema, table)
                output_file = table_path / accessor.filename
                output_file.write_text(content)

            tables_synced += 1
            progress.update(table_task, advance=1)

        progress.update(schema_task, advance=1)

    return schemas_synced, tables_synced


def sync_snowflake(
    db_config,
    base_path: Path,
    progress: Progress,
    accessors: list[DataAccessor],
) -> tuple[int, int]:
    """Sync Snowflake database schema to markdown files.

    Args:
            db_config: The database configuration
            base_path: Base output path
            progress: Rich progress instance
            accessors: List of data accessors to run

    Returns:
            Tuple of (schemas_synced, tables_synced)
    """
    conn = db_config.connect()
    db_path = base_path / "type=snowflake" / f"database={db_config.database}"

    schemas_synced = 0
    tables_synced = 0

    if db_config.schema:
        schemas = [db_config.schema]
    else:
        schemas = conn.list_databases()

    schema_task = progress.add_task(
        f"[dim]{db_config.name}[/dim]",
        total=len(schemas),
    )

    for schema in schemas:
        try:
            all_tables = conn.list_tables(database=schema)
        except Exception:
            progress.update(schema_task, advance=1)
            continue

        # Filter tables based on include/exclude patterns
        tables = [t for t in all_tables if db_config.matches_pattern(schema, t)]

        # Skip schema if no tables match
        if not tables:
            progress.update(schema_task, advance=1)
            continue

        schema_path = db_path / f"schema={schema}"
        schema_path.mkdir(parents=True, exist_ok=True)
        schemas_synced += 1

        table_task = progress.add_task(
            f"  [cyan]{schema}[/cyan]",
            total=len(tables),
        )

        for table in tables:
            table_path = schema_path / f"table={table}"
            table_path.mkdir(parents=True, exist_ok=True)

            for accessor in accessors:
                content = accessor.generate(conn, schema, table)
                output_file = table_path / accessor.filename
                output_file.write_text(content)

            tables_synced += 1
            progress.update(table_task, advance=1)

        progress.update(schema_task, advance=1)

    return schemas_synced, tables_synced


def sync_postgres(
    db_config,
    base_path: Path,
    progress: Progress,
    accessors: list[DataAccessor],
) -> tuple[int, int]:
    """Sync PostgreSQL database schema to markdown files.

    Args:
            db_config: The database configuration
            base_path: Base output path
            progress: Rich progress instance
            accessors: List of data accessors to run

    Returns:
            Tuple of (schemas_synced, tables_synced)
    """
    conn = db_config.connect()
    db_path = base_path / "type=postgres" / f"database={db_config.database}"

    schemas_synced = 0
    tables_synced = 0

    if db_config.schema_name:
        schemas = [db_config.schema_name]
    else:
        schemas = conn.list_databases()

    schema_task = progress.add_task(
        f"[dim]{db_config.name}[/dim]",
        total=len(schemas),
    )

    for schema in schemas:
        try:
            all_tables = conn.list_tables(database=schema)
        except Exception:
            progress.update(schema_task, advance=1)
            continue

        # Filter tables based on include/exclude patterns
        tables = [t for t in all_tables if db_config.matches_pattern(schema, t)]

        # Skip schema if no tables match
        if not tables:
            progress.update(schema_task, advance=1)
            continue

        schema_path = db_path / f"schema={schema}"
        schema_path.mkdir(parents=True, exist_ok=True)
        schemas_synced += 1

        table_task = progress.add_task(
            f"  [cyan]{schema}[/cyan]",
            total=len(tables),
        )

        for table in tables:
            table_path = schema_path / f"table={table}"
            table_path.mkdir(parents=True, exist_ok=True)

            for accessor in accessors:
                content = accessor.generate(conn, schema, table)
                output_file = table_path / accessor.filename
                output_file.write_text(content)

            tables_synced += 1
            progress.update(table_task, advance=1)

        progress.update(schema_task, advance=1)

    return schemas_synced, tables_synced


def sync_databases(databases: list, base_path: Path) -> tuple[int, int]:
    """Sync all configured databases.

    Args:
            databases: List of database configurations
            base_path: Base path where database schemas are stored

    Returns:
            Tuple of (total_datasets, total_tables) synced
    """
    if not databases:
        console.print("\n[dim]No databases configured[/dim]")
        return 0, 0

    total_datasets = 0
    total_tables = 0

    console.print("\n[bold cyan]üóÑÔ∏è  Syncing Databases[/bold cyan]")
    console.print(f"[dim]Location:[/dim] {base_path.absolute()}\n")

    with Progress(
        SpinnerColumn(style="dim"),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(bar_width=30, style="dim", complete_style="cyan", finished_style="green"),
        TaskProgressColumn(),
        console=console,
        transient=False,
    ) as progress:
        for db in databases:
            # Get accessors from database config
            db_accessors = get_accessors(db.accessors)
            accessor_names = [a.filename.replace(".md", "") for a in db_accessors]

            try:
                console.print(f"[dim]{db.name} accessors:[/dim] {', '.join(accessor_names)}")
                if db.type == "bigquery":
                    datasets, tables = sync_bigquery(db, base_path, progress, db_accessors)
                    total_datasets += datasets
                    total_tables += tables
                elif db.type == "duckdb":
                    schemas, tables = sync_duckdb(db, base_path, progress, db_accessors)
                    total_datasets += schemas
                    total_tables += tables
                elif db.type == "databricks":
                    console.print(f"[dim]{db.name} accessors:[/dim] {', '.join(accessor_names)}")
                    schemas, tables = sync_databricks(db, base_path, progress, db_accessors)
                    total_datasets += schemas
                    total_tables += tables
                elif db.type == "snowflake":
                    console.print(f"[dim]{db.name} accessors:[/dim] {', '.join(accessor_names)}")
                    schemas, tables = sync_snowflake(db, base_path, progress, db_accessors)
                    total_datasets += schemas
                    total_tables += tables
                elif db.type == "postgres":
                    console.print(f"[dim]{db.name} accessors:[/dim] {', '.join(accessor_names)}")
                    schemas, tables = sync_postgres(db, base_path, progress, db_accessors)
                    total_datasets += schemas
                    total_tables += tables
                else:
                    console.print(f"[yellow]‚ö† Unsupported database type: {db.type}[/yellow]")
            except Exception as e:
                console.print(f"[bold red]‚úó[/bold red] Failed to sync {db.name}: {e}")

    return total_datasets, total_tables


def remove_unused_databases(databases: List, db_root: Path):
    """Remove databases that are not present in the config file."""

    valid_db_folders_by_type: Dict[str, set] = defaultdict(set)

    for db in databases:
        type_folder = f"type={db.type}"
        db_identifier = get_database_identifier(db)
        db_folder = f"database={db_identifier}"

        valid_db_folders_by_type[type_folder].add(db_folder)

    for type_dir in db_root.iterdir():
        if not type_dir.is_dir():
            continue

        type_folder_name = type_dir.name

        # Remove entire type directory if it doesn't exist in nao_config
        if type_folder_name not in valid_db_folders_by_type:
            shutil.rmtree(type_dir)
            console.print(f"[yellow]üóë Removed unused database type:[/yellow] {type_dir}")
            continue

        valid_db_folders = valid_db_folders_by_type[type_folder_name]

        # Remove unused database folders if it doesn't exist in nao_config
        for db_dir in type_dir.iterdir():
            if not db_dir.is_dir():
                continue

            if db_dir.name not in valid_db_folders:
                shutil.rmtree(db_dir)
                console.print(f"[yellow]üóë Removed unused database:[/yellow] {type_folder_name}/{db_dir.name}")


def get_database_identifier(db):
    if db.type == "bigquery":
        return db.project_id

    elif db.type == "duckdb":
        if db.path == ":memory:":
            return "memory"
        return Path(db.path).stem

    elif db.type == "databricks":
        return db.catalog or "main"

    elif db.type == "snowflake":
        return db.database

    elif db.type == "postgres":
        return db.database
